# -*- coding: utf-8 -*-
"""DecisionTree.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jmf6SbQmLBE1RWMe44Osok2aPgPWE2oC

In this file, we work with one dataset that you need to upload to your session:


*   NYT.csv
"""

import pandas as pd
df = pd.read_csv("NYT.csv")

df

"""## Building and Visualizing a Decision Tree

Now let’s see how we can use a decision tree to predict whether a customer subscribes. The first step is to prepare our data.

Most machine learning algorithms, including decision trees, work best with numeric inputs. If our dataset has categorical variables (for instance, something like “topic preference” with values such as politics, lifestyle, or sports), the algorithm does know how to handle those directly. To solve this, we convert categories into numeric indicators using `get_dummies` function. This way, each category becomes its own column with values of 0 or 1, making the data tree-friendly or in general machine learning-friendly.

Once the features are ready, we can create and fit our tree. Here we use `DecisionTreeClassifier` from scikit-learn. The key parameter to notice is `ccp_alpha`, which controls how much we prune the tree. A smaller alpha means the tree can keep splitting and become very complex, potentially overfitting the training data. A larger alpha prunes more aggressively, producing a simpler, more generalizable tree. By setting this parameter, we are telling the model to balance between capturing patterns and keeping the tree interpretable.

After fitting the tree to our data, the next natural step is to see what it looks like. Visualization is especially powerful with decision trees because it shows the sequence of “if–then” rules the model is using. For small to medium-sized trees, the picture can give us real intuition: which variables the tree split on first, how it divided customers, and what leads to a “Yes” versus a “No.” We use `plot_tree` function to generate this visualization, add some formatting so the splits are easier to read, and then display the result. The outcome is a clear flowchart of decisions that helps us both understand and explain the model.

Once the tree has been trained, it is helpful to look inside and see what rules it actually learned. That is the purpose of the `plot_tree` function. By default, `plot_tree` would just show a bare tree, but here we add several arguments to make the picture more informative and readable.

- `feature_names=X.columns`: this tells the plot to display our actual variable names (the column names of the dataset) at each split, rather than just generic labels like “X[0]” or “X[1].” That way, we can see which customer attributes the tree is using to divide the data.  

- `class_names=["No","Yes"]`: our target variable subscribed was coded numerically (0 for not subscribed, 1 for subscribed). This option maps those numeric values to meaningful labels. The first entry in the list (“No”) corresponds to class 0, and the second (“Yes”) corresponds to class 1.  

- `filled=True`: fills each box with a color that reflects the majority class in that node. The deeper the shade, the more confident the prediction. This makes it visually clear whether a branch leans more toward “No” or “Yes.”  

- `rounded=True`: just smooths the corners of the boxes so the diagram looks cleaner.  

- `proportion=True`: instead of showing raw counts (like “123 customers in this node”), it shows proportions relative to the whole dataset. This makes it easier to compare the size of nodes.  

- `impurity=False`: hides the impurity measure from each box. While impurity is useful for algorithm development, it can clutter the diagram when we are just trying to explain the decision path.  

- `fontsize=9`: controls the size of the text in the boxes so that everything fits nicely on the figure.  

Together, these options turn a plain decision tree into an interpretable flowchart. We see which variables matter most (because they appear higher up in the tree), how the tree splits customers into different paths, and what proportion of each branch ends up subscribing or not. This makes the model not just a predictive tool but also a way to communicate insights about the data.
"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
import pandas as pd
import matplotlib.pyplot as plt

# Dealing with categorical variables
X = df.drop(columns=["subscribed", "customer_id"]).copy()
y = df["subscribed"]
X = pd.get_dummies(X)

# Fitting a tree
model = DecisionTreeClassifier(ccp_alpha=0.0003,random_state=0)
model.fit(X, y)

# 3) Visualize the tree
plt.figure(figsize=(14,8))
plot_tree(
    model,
    feature_names=X.columns,
    class_names=["No","Yes"],
    filled=True, rounded=True,
    proportion=True, impurity=False,
    fontsize=9
)
plt.show()

"""Now that we have trained the model, we can easily get predictions similar to what we did in neural networks. In other words, we use the `.predict_proba()` function to get probability predictions."""

df['preds'] = model.predict_proba(X)[:,1]
df

"""Now we want to consider the case where a specific variables is dropped from the analysis:"""

from sklearn.tree import DecisionTreeClassifier, plot_tree
import pandas as pd
import matplotlib.pyplot as plt

# Dealing with categorical variables
X = df.drop(columns=["subscribed", "customer_id", "topic_pref", "preds"]).copy()
y = df["subscribed"]
X = pd.get_dummies(X)

# Fitting a tree
model = DecisionTreeClassifier(ccp_alpha=0.0003,random_state=0)
model.fit(X, y)

# 3) Visualize the tree
plt.figure(figsize=(14,8))
plot_tree(
    model,
    feature_names=X.columns,
    class_names=["No","Yes"],
    filled=True, rounded=True,
    proportion=True, impurity=False,
    fontsize=9
)
plt.show()

"""**Question:** What do you notice between the above tree and the tree with all variables present?

Let's dive deeper by considering the range of values for one of the variable: the number of minutes that the reader has spent on the platform in the past 90 days

## Model Performance of the Tree
"""

from sklearn.metrics import roc_curve

y_true = y
y_score = model.predict_proba(X)[:,1]
# ROC
fpr, tpr, _ = roc_curve(y_true, y_score)         # thresholds not needed for plot

# Plot
import matplotlib.pyplot as plt

plt.plot(fpr, tpr, label='Decision Tree')
plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(alpha=0.25)
plt.show()

"""## Comparing against a Logit Model"""

import statsmodels.formula.api as smf

model_logit = smf.logit('subscribed ~ articles_read_3m + device_type + topic_pref + paywall_hit + newsletter_signup + referral + reading_minutes_90d', data=df).fit()

print(model_logit.summary())

"""**Question:** What does the logit result tell you about the subscription behavior? Does that make sense to you?"""

# Comparing ROC curves of Logit and decision tree
from sklearn.metrics import roc_curve

y_true  = y
y_score1 = model_logit.predict(df)
fpr1, tpr1, _ = roc_curve(y_true, y_score1)

y_score2 = model.predict_proba(X)[:,1]
fpr2, tpr2, _ = roc_curve(y_true, y_score2)

import matplotlib.pyplot as plt

plt.plot(fpr1, tpr1, label='Logit')
plt.plot(fpr2, tpr2, label='Decision Trees')
plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve (All Data)')
plt.legend()
plt.grid()
plt.show()

# Let's look at the AUC of both models
from sklearn.metrics import auc

auc_logit = auc(fpr1, tpr1)
print(auc_logit)

auc_tree = auc(fpr2, tpr2)
print(auc_tree)

"""As we mentioned in model performance, we should always aim for testing model prediction only on the valiaditon sample. We have practiced this before in the Model Performance session on how to divide the data into training and validation sample."""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv("NYT.csv")
TARGET = "subscribed"


train_idx, val_idx = train_test_split(
    df.index,
    test_size=0.30,
    random_state=0,
    stratify=df[TARGET]
)


df["train_flag"] = np.where(df.index.isin(train_idx), 1, 0)

df_train = df[df["train_flag"] == 1].copy()   # rows where train_flag = 1
df_val   = df[df["train_flag"] == 0].copy()   # rows where train_flag = 0

# Fitting Logit on training sample and gettind predictions on validation sample
import statsmodels.formula.api as smf
model_logit = smf.logit('subscribed ~ articles_read_3m + device_type + topic_pref + paywall_hit + newsletter_signup + referral + reading_minutes_90d', data=df_train).fit()
y_true  = df_val[TARGET]
y_score1 = model_logit.predict(df_val)
fpr1, tpr1, _ = roc_curve(y_true, y_score1)



# Fitting decision tree on training sample and gettind predictions on validation sample
X_train = df_train.drop(columns=["subscribed", "customer_id"]).copy()
y_train = df_train["subscribed"]
X_train = pd.get_dummies(X_train)

X_val = df_val.drop(columns=["subscribed", "customer_id"]).copy()
X_val = pd.get_dummies(X_val).reindex(columns=X_train.columns, fill_value=0)
# Reindex ensures train and validation have the same columns. Why? In rare cases, some categories might only appear in one split (e.g., train has 5 product types, but val only sees 4). If we call get_dummies separately, the two datasets could end up with different numbers of columns — which breaks the model, since it expects the same inputs at training and prediction. Reindex forces val to match the train columns, and any "missing" categories are just filled with zeros (meaning they didn’t appear).

model_tree = DecisionTreeClassifier(ccp_alpha=0.0003,random_state=0)
model_tree.fit(X_train, y_train)

y_score2 = model_tree.predict_proba(X_val)[:,1]
fpr2, tpr2, _ = roc_curve(y_true, y_score2)


import matplotlib.pyplot as plt

plt.plot(fpr1, tpr1, label='Logit')
plt.plot(fpr2, tpr2, label='Decision Trees')
plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Validation)')
plt.legend()
plt.grid()
plt.show()

"""## Impact of Complexity Parameter

What if we lower our complexity parameters? Yes the tree is worse than loigistic regression, but maybe that is not the model's fault. Maybe we have been just too conservative with the complexity parameter. What happens when we decide to run a tree without specifiying such parameter?
"""

df

from sklearn.tree import DecisionTreeClassifier, plot_tree
import pandas as pd
import matplotlib.pyplot as plt

# Dealing with categorical variables
X = df.drop(columns=["subscribed", "customer_id", "train_flag"]).copy()
y = df["subscribed"]
X = pd.get_dummies(X)

# Fitting a tree
model = DecisionTreeClassifier(random_state=0)
model.fit(X, y)

# 3) Visualize the tree
plt.figure(figsize=(14,8))
plot_tree(
    model,
    feature_names=X.columns,
    class_names=["No","Yes"],
    filled=True, rounded=True,
    proportion=True, impurity=False,
    fontsize=9
)
plt.show()

"""This tree of course is not readable, but the point that we care the most is prediction not necessarily reading inside each box individually! Let's see what happens when we try to get predictions?"""

df['pred'] = model.predict_proba(X)[:,1]
df

"""**Question:** What can you notice about the predictions?"""

# Let's look at the ROC curve
from sklearn.metrics import roc_curve

y_true = y
y_score = model.predict_proba(X)[:,1]
# ROC
fpr, tpr, _ = roc_curve(y_true, y_score)         # thresholds not needed for plot

# Plot
import matplotlib.pyplot as plt

plt.plot(fpr, tpr, label='Decision Tree')
plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve (No Complexity Parameter)')
plt.legend()
plt.grid(alpha=0.25)
plt.show()

"""What about looking at the validatoin sample prediciton and comparing it to logit?"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv("NYT.csv")
TARGET = "subscribed"


train_idx, val_idx = train_test_split(
    df.index,
    test_size=0.30,
    random_state=0,
    stratify=df[TARGET]
)


df["train_flag"] = np.where(df.index.isin(train_idx), 1, 0)

df_train = df[df["train_flag"] == 1].copy()   # rows where train_flag = 1
df_val   = df[df["train_flag"] == 0].copy()   # rows where train_flag = 0

# Fitting Logit on training sample and gettind predictions on validation sample
import statsmodels.formula.api as smf
model_logit = smf.logit('subscribed ~ articles_read_3m + device_type + topic_pref + paywall_hit + newsletter_signup + referral + reading_minutes_90d', data=df_train).fit()
y_true  = df_val[TARGET]
y_score1 = model_logit.predict(df_val)
fpr1, tpr1, _ = roc_curve(y_true, y_score1)



# Fitting decision tree on training sample and gettind predictions on validation sample
X_train = df_train.drop(columns=["subscribed", "customer_id"]).copy()
y_train = df_train["subscribed"]
X_train = pd.get_dummies(X_train)

X_val = df_val.drop(columns=["subscribed", "customer_id"]).copy()
X_val = pd.get_dummies(X_val).reindex(columns=X_train.columns, fill_value=0)
# Reindex ensures train and validation have the same columns. Why? In rare cases, some categories might only appear in one split (e.g., train has 5 product types, but val only sees 4). If we call get_dummies separately, the two datasets could end up with different numbers of columns — which breaks the model, since it expects the same inputs at training and prediction. Reindex forces val to match the train columns, and any "missing" categories are just filled with zeros (meaning they didn’t appear).

model_tree = DecisionTreeClassifier(random_state=0)
model_tree.fit(X_train, y_train)

y_score2 = model_tree.predict_proba(X_val)[:,1]
fpr2, tpr2, _ = roc_curve(y_true, y_score2)


import matplotlib.pyplot as plt

plt.plot(fpr1, tpr1, label='Logit')
plt.plot(fpr2, tpr2, label='Decision Trees - No Complexity Parameter')
plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Validation)')
plt.legend()
plt.grid()
plt.show()

"""**Question:** What do you notice from the comparison between ROC curves of training and validation samples for the case of no-complexity parameter?

# Random Forest and Boosting

Implementing Random Forest and XGBoost is simple in python, they only require one line of code similar to many models that we have covered so far in this course.
"""

# Random Forest (1 line to train + predict probabilities)
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=0).fit(X_train, y_train)     #So instead of typing DecisionTreeClassifier we type RandomForestClassifier. That is the only difference.
rf_preds = rf.predict_proba(X_val)[:,1]

rf_preds

# XGBoost (1 line to train + predict probabilities)
from xgboost import XGBClassifier
xgb = XGBClassifier(random_state=0).fit(X_train, y_train)           #So instead of typing DecisionTreeClassifier we type XGBClassifier. That is the only difference.
xgb_preds = xgb.predict_proba(X_val)[:,1]

xgb_preds

"""## Comparing All the Models"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

df = pd.read_csv("NYT.csv")
TARGET = "subscribed"


train_idx, val_idx = train_test_split(
    df.index,
    test_size=0.30,
    random_state=0,
    stratify=df[TARGET]
)


df["train_flag"] = np.where(df.index.isin(train_idx), 1, 0)

df_train = df[df["train_flag"] == 1].copy()   # rows where train_flag = 1
df_val   = df[df["train_flag"] == 0].copy()   # rows where train_flag = 0

# Fitting Logit on training sample and gettind predictions on validation sample
import statsmodels.formula.api as smf
model_logit = smf.logit('subscribed ~ articles_read_3m + device_type + topic_pref + paywall_hit + newsletter_signup + referral + reading_minutes_90d', data=df_train).fit()
y_true  = df_val[TARGET]
y_score1 = model_logit.predict(df_val)
fpr1, tpr1, _ = roc_curve(y_true, y_score1)



# Fitting decision tree on training sample and gettind predictions on validation sample
X_train = df_train.drop(columns=["subscribed", "customer_id"]).copy()
y_train = df_train["subscribed"]
X_train = pd.get_dummies(X_train)

X_val = df_val.drop(columns=["subscribed", "customer_id"]).copy()
X_val = pd.get_dummies(X_val).reindex(columns=X_train.columns, fill_value=0)
# Reindex ensures train and validation have the same columns. Why? In rare cases, some categories might only appear in one split (e.g., train has 5 product types, but val only sees 4). If we call get_dummies separately, the two datasets could end up with different numbers of columns — which breaks the model, since it expects the same inputs at training and prediction. Reindex forces val to match the train columns, and any "missing" categories are just filled with zeros (meaning they didn’t appear).

model_tree = DecisionTreeClassifier(ccp_alpha= 0.0003, random_state=0)
model_tree.fit(X_train, y_train)

y_score2 = model_tree.predict_proba(X_val)[:,1]
fpr2, tpr2, _ = roc_curve(y_true, y_score2)


# Fitting a random forest model
model_rf = RandomForestClassifier(random_state=0).fit(X_train, y_train)
y_score3 = model_rf.predict_proba(X_val)[:,1]
fpr3, tpr3, _ = roc_curve(y_true, y_score3)


# Fitting an XGBoost model

model_xgb = XGBClassifier(random_state=0).fit(X_train, y_train)
y_score4 = model_xgb.predict_proba(X_val)[:,1]
fpr4, tpr4, _ = roc_curve(y_true, y_score4)

# Plotting all ROC curves
import matplotlib.pyplot as plt

plt.plot(fpr1, tpr1, label='Logit')
plt.plot(fpr2, tpr2, label='Decision Trees')
plt.plot(fpr3, tpr3, label='Random Forest')
plt.plot(fpr4, tpr4, label='XGBoost')

plt.plot([0,1], [0,1], 'k--', label='Random')
plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')
plt.title('ROC Curve (Validation)')
plt.legend()
plt.grid()
plt.show()

"""***Lesson of the Day: Sometimes Noting Beats Simplicity!***"""